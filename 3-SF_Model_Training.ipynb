{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies ###\n",
    "\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.snowpark.functions import col, count, when, mean, lit, corr\n",
    "from snowflake.snowpark.types import StringType, LongType, DecimalType\n",
    "\n",
    "\n",
    "# Snowflake ML\n",
    "from snowflake.ml.modeling.metrics.correlation import correlation\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from snowflake.ml.modeling.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from snowflake.ml.modeling.metrics import accuracy_score, precision_score\n",
    "\n",
    "# Workflow\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Secured connection to Snowflake ###\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "database = 'FRAUD_DETECT_DB'\n",
    "schema = 'FRAUD_DETECT_SM'\n",
    "table = 'FRAUD_DATA_CLEANED'\n",
    "input_tbl = f\"{database}.{schema}.{table}\"\n",
    "fraud_data = session.table(input_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Categorical and Numerical cols ###\n",
    "cat_cols = [field.name for field in fraud_data.schema.fields if not isinstance(field.datatype,(LongType,DecimalType))]\n",
    "num_cols = [field.name for field in fraud_data.schema.fields if isinstance(field.datatype,LongType)]\n",
    "one_hot_output_cols = [f\"{col}_encoded\" for col in cat_cols ]\n",
    "min_max_output_cols = [f\"{col}_scaled\" for col in num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline for data preprocessing ###\n",
    "log_reg_pipeline = Pipeline(steps=\n",
    "                [(\n",
    "                    \"OneHotEncoder\",\n",
    "                    OneHotEncoder(\n",
    "                        input_cols = cat_cols,\n",
    "                        output_cols = one_hot_output_cols\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    \"MinMaxScale\",\n",
    "                    MinMaxScaler(\n",
    "                        input_cols = num_cols,\n",
    "                        output_cols = min_max_output_cols\n",
    "                    )\n",
    "                )])\n",
    "\n",
    "PIPELINE_FILE = 'log_reg_pipeline.joblib'\n",
    "joblib.dump(log_reg_pipeline,PIPELINE_FILE) # to serialize job\n",
    "put_result = session.file.put(PIPELINE_FILE, '@FRAUD_DETECT_DB.FRAUD_DETECT_SM.INTERNAL_FRAUD_STG', overwrite=True) # job staged to SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonin/miniconda3/envs/fd_3.10/lib/python3.10/site-packages/snowflake/ml/modeling/pipeline/pipeline.py:425: UserWarning: Warning: The Decimal(38, 18) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "  snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
      "/home/antonin/miniconda3/envs/fd_3.10/lib/python3.10/site-packages/snowflake/ml/modeling/pipeline/pipeline.py:479: UserWarning: Warning: The Decimal(38, 18) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "  snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
      "/home/antonin/miniconda3/envs/fd_3.10/lib/python3.10/site-packages/snowflake/ml/modeling/pipeline/pipeline.py:479: UserWarning: Warning: The Decimal(38, 18) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "  snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n"
     ]
    }
   ],
   "source": [
    "### Train Test Split ###\n",
    "train_df,test_df = fraud_data.random_split(weights=[0.8,0.2],seed=42)\n",
    "columns_to_remove = cat_cols + num_cols # keeping preprocessed columns only\n",
    "train_df = log_reg_pipeline.fit(train_df).transform(train_df).drop(columns_to_remove)\n",
    "test_df = log_reg_pipeline.transform(test_df).drop(columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we were using the 'newton_cholesky' method in the local training, let's use the same penalty (l2) for the cloud training, still using the same random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error object of type 'NoneType' has no len() when trying to read default values from function: <function SnowparkModelTrainer._build_fit_wrapper_sproc.<locals>.fit_wrapper_function at 0x70d5eb5601f0>. Proceeding without creating optional arguments\n",
      "The version of package 'snowflake-snowpark-python' in the local environment is 1.22.1, which does not fit the criteria for the requirement 'snowflake-snowpark-python'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    }
   ],
   "source": [
    "### Model Definition ###\n",
    "# Logistic Regression\n",
    "feature_cols = train_df.columns.remove('FRAUD_BOOL_SCALED')\n",
    "label_cols = 'FRAUD_BOOL_SCALED'\n",
    "output_cols = ['PREDICTED_FRAUD']\n",
    "\n",
    "log_reg_model = LogisticRegression(random_state=42,\n",
    "                                   solver='newton-cholesky',\n",
    "                                   input_cols = feature_cols,\n",
    "                                   label_cols =label_cols,\n",
    "                                   output_cols =output_cols) # default penalty : 'l2'\n",
    "\n",
    "\n",
    "log_reg_model.fit(train_df)\n",
    "predictions = log_reg_model.predict(test_df) # test_df with predictions as extra column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.791009"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df=predictions,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD') # 79%, approximately same accuracy as before (with the same random state) !!\n",
    "precision_score(df=predictions,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's go further and train a cross validated model ###\n",
    "# log_reg_model_cv = LogisticRegressionCV(random_state=42,\n",
    "#                                      solver='newton-cholesky',\n",
    "#                                      input_cols= feature_cols,\n",
    "#                                      label_cols = label_cols,\n",
    "#                                      output_cols = output_cols,\n",
    "#                                      cv=10)\n",
    "\n",
    "# log_reg_model_cv.fit(train_df)\n",
    "# predictions_cv = log_reg_model.predict(test_df) # test_df with predictions as extra column # TOO LONG ... MORE THAN 5 MINS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
