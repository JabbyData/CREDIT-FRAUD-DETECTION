{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies ###\n",
    "\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.snowpark.functions import col, count, when, mean, lit, corr\n",
    "from snowflake.snowpark.types import StringType, LongType, DecimalType\n",
    "\n",
    "\n",
    "# Snowflake ML\n",
    "from snowflake.ml.modeling.metrics.correlation import correlation\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from snowflake.ml.modeling.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from snowflake.ml.modeling.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Workflow\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Secured connection to Snowflake ###\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "database = 'FRAUD_DETECT_DB'\n",
    "schema = 'FRAUD_DETECT_SM'\n",
    "table = 'FRAUD_DATA_CLEANED'\n",
    "input_tbl = f\"{database}.{schema}.{table}\"\n",
    "fraud_data = session.table(input_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Categorical and Numerical cols ###\n",
    "cat_cols = [field.name for field in fraud_data.schema.fields if not isinstance(field.datatype,(LongType,DecimalType))]\n",
    "num_cols = [field.name for field in fraud_data.schema.fields if isinstance(field.datatype,LongType)]\n",
    "one_hot_output_cols = [f\"{col}_encoded\" for col in cat_cols ]\n",
    "min_max_output_cols = [f\"{col}_scaled\" for col in num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline for data preprocessing ###\n",
    "log_reg_pipeline = Pipeline(steps=\n",
    "                [(\n",
    "                    \"OneHotEncoder\",\n",
    "                    OneHotEncoder(\n",
    "                        input_cols = cat_cols,\n",
    "                        output_cols = one_hot_output_cols\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    \"MinMaxScale\",\n",
    "                    MinMaxScaler(\n",
    "                        input_cols = num_cols,\n",
    "                        output_cols = min_max_output_cols\n",
    "                    )\n",
    "                )])\n",
    "\n",
    "PIPELINE_FILE = 'jobs/log_reg_pipeline.joblib'\n",
    "joblib.dump(log_reg_pipeline,PIPELINE_FILE) # to serialize job\n",
    "put_result = session.file.put(PIPELINE_FILE, '@FRAUD_DETECT_DB.FRAUD_DETECT_SM.INTERNAL_FRAUD_STG', overwrite=True) # job staged to SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonin/miniconda3/envs/fd_3.10/lib/python3.10/site-packages/snowflake/ml/modeling/pipeline/pipeline.py:425: UserWarning: Warning: The Decimal(38, 18) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "  snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
      "/home/antonin/miniconda3/envs/fd_3.10/lib/python3.10/site-packages/snowflake/ml/modeling/pipeline/pipeline.py:479: UserWarning: Warning: The Decimal(38, 18) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "  snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
      "/home/antonin/miniconda3/envs/fd_3.10/lib/python3.10/site-packages/snowflake/ml/modeling/pipeline/pipeline.py:479: UserWarning: Warning: The Decimal(38, 18) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "  snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n"
     ]
    }
   ],
   "source": [
    "### Train Test Split + pipeline processing ###\n",
    "\n",
    "train_df,test_df = fraud_data.random_split(weights=[0.8,0.2],seed=42)\n",
    "columns_to_remove = cat_cols + num_cols # keeping preprocessed columns only\n",
    "\n",
    "session.file.get('@INTERNAL_FRAUD_STG/log_reg_pipeline.joblib.gz', 'jobs')\n",
    "log_reg_pipeline = joblib.load('jobs/log_reg_pipeline.joblib.gz')\n",
    "\n",
    "\n",
    "train_df = log_reg_pipeline.fit(train_df).transform(train_df).drop(columns_to_remove)\n",
    "test_df = log_reg_pipeline.transform(test_df).drop(columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we were using the 'newton_cholesky' method in the local training, let's use the same penalty (l2) for the cloud training, still using the same random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error object of type 'NoneType' has no len() when trying to read default values from function: <function SnowparkModelTrainer._build_fit_wrapper_sproc.<locals>.fit_wrapper_function at 0x7b5439f6fa30>. Proceeding without creating optional arguments\n",
      "The version of package 'snowflake-snowpark-python' in the local environment is 1.22.1, which does not fit the criteria for the requirement 'snowflake-snowpark-python'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    }
   ],
   "source": [
    "### Model Definition ###\n",
    "# Logistic Regression\n",
    "feature_cols = train_df.columns.remove('FRAUD_BOOL_SCALED')\n",
    "label_cols = 'FRAUD_BOOL_SCALED'\n",
    "output_cols = ['PREDICTED_FRAUD']\n",
    "\n",
    "log_reg_model = LogisticRegression(random_state=42,\n",
    "                                   solver='newton-cholesky',\n",
    "                                   input_cols = feature_cols,\n",
    "                                   label_cols =label_cols,\n",
    "                                   output_cols =output_cols) # default penalty : 'l2'\n",
    "\n",
    "\n",
    "log_reg_model.fit(train_df)\n",
    "train_predictions = log_reg_model.predict(train_df) # train_df with predictions as extra column\n",
    "test_predictions = log_reg_model.predict(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model. As a reminder we are using 4 metrics :\n",
    "\n",
    "- **Accuracy** : Overall efficiency of the model (not very informative)\n",
    "- **Precision** : Details how much the model correctly identifies true frauds\n",
    "- **Recall** : Highlights how well the model ecompasses true fraud operations\n",
    "- **F1 measure** : Evaluation score of the model (harmonic mean between precision and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model scores :\n",
      "Train Accuracy : 0.791547\n",
      "Train precision : 0.7965915359304421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error object of type 'NoneType' has no len() when trying to read default values from function: <class 'snowflake.ml.modeling.metrics.classification._register_multilabel_confusion_matrix_computer.<locals>.MultilabelConfusionMatrixComputer'>. Proceeding without creating optional arguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train recall : 0.783299701103036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error object of type 'NoneType' has no len() when trying to read default values from function: <class 'snowflake.ml.modeling.metrics.classification._register_multilabel_confusion_matrix_computer.<locals>.MultilabelConfusionMatrixComputer'>. Proceeding without creating optional arguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 : 0.7898897055307885\n",
      "---------------------------------------------------------------\n",
      "Test Accuracy : 0.79142\n",
      "Test precision : 0.7961711769571852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error object of type 'NoneType' has no len() when trying to read default values from function: <class 'snowflake.ml.modeling.metrics.classification._register_multilabel_confusion_matrix_computer.<locals>.MultilabelConfusionMatrixComputer'>. Proceeding without creating optional arguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall : 0.7823672265895195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error object of type 'NoneType' has no len() when trying to read default values from function: <class 'snowflake.ml.modeling.metrics.classification._register_multilabel_confusion_matrix_computer.<locals>.MultilabelConfusionMatrixComputer'>. Proceeding without creating optional arguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 : 0.7892088456092816\n"
     ]
    }
   ],
   "source": [
    "def model_evaluation(train_df,test_df):\n",
    "    \"\"\" \n",
    "    Display model evaluation metrics (classification)\n",
    "    Input : \n",
    "        - train_df : snowflake dataframe on which the model is trained\n",
    "        - test_df : snowflake dataframe on which the model is tested\n",
    "    \"\"\"\n",
    "    print('Model scores :')\n",
    "    print('Train Accuracy :',accuracy_score(df=train_df,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD'))\n",
    "    print('Train precision :',precision_score(df=train_df,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD'))\n",
    "    print('Train recall :',recall_score(df=train_df,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD'))\n",
    "    print('Train F1 :',f1_score(df=train_df,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD'))\n",
    "\n",
    "    print('---------------------------------------------------------------')\n",
    "\n",
    "    print('Test Accuracy :',accuracy_score(df=test_df,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD'))\n",
    "    print('Test precision :',precision_score(df=test_df,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD'))\n",
    "    print('Test recall :',recall_score(df=test_df,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD'))\n",
    "    print('Test F1 :',f1_score(df=test_df,y_true_col_names='FRAUD_BOOL_SCALED',y_pred_col_names='PREDICTED_FRAUD'))\n",
    "\n",
    "model_evaluation(train_predictions,test_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's go further and train a cross validated model ###\n",
    "# log_reg_model_cv = LogisticRegressionCV(random_state=42,\n",
    "#                                      solver='newton-cholesky',\n",
    "#                                      input_cols= feature_cols,\n",
    "#                                      label_cols = label_cols,\n",
    "#                                      output_cols = output_cols,\n",
    "#                                      cv=10)\n",
    "\n",
    "# log_reg_model_cv.fit(train_df)\n",
    "# predictions_cv = log_reg_model.predict(test_df) # test_df with predictions as extra column # TOO LONG ... MORE THAN 5 MINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
